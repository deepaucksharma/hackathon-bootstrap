/**
 * Enhanced Kafka Data Collector
 * 
 * Advanced collector that aligns with nri-kafka's JMX-based approach while
 * providing structured metric collection, concurrent processing, and
 * comprehensive error handling.
 * 
 * Features:
 * - JMX-based metric collection from brokers
 * - Per-broker and per-topic metrics
 * - Concurrent collection using worker pools
 * - Connection verification and health checks
 * - Structured metric definitions
 * - Comprehensive error handling and retries
 */

const InfraAgentCollector = require('./infra-agent-collector');
const { DataCollectionPool } = require('../../core/workers/worker-pool');
const { 
  getAllMetricDefinitions, 
  getMetricsByStrategy,
  COLLECTION_STRATEGIES 
} = require('../../core/metrics/metric-definitions');
const { logger } = require('../../core/utils/logger');
const chalk = require('chalk');

class EnhancedKafkaCollector extends InfraAgentCollector {
  constructor(config = {}) {
    super(config);
    
    this.collectionConfig = {
      brokerWorkerPoolSize: config.brokerWorkerPoolSize || 5,
      topicWorkerPoolSize: config.topicWorkerPoolSize || 8,
      consumerWorkerPoolSize: config.consumerWorkerPoolSize || 3,
      enableDetailedTopicMetrics: config.enableDetailedTopicMetrics !== false,
      enableConsumerLagCollection: config.enableConsumerLagCollection !== false,
      connectionTimeout: config.connectionTimeout || 30000,
      metricValidation: config.metricValidation !== false,
      retryStrategy: {
        maxRetries: config.maxRetries || 3,
        retryDelay: config.retryDelay || 2000,
        backoffMultiplier: config.backoffMultiplier || 1.5
      }
    };
    
    // Initialize worker pools
    this.brokerPool = new DataCollectionPool({
      name: 'BrokerMetricsPool',
      poolSize: this.collectionConfig.brokerWorkerPoolSize,
      taskTimeout: 45000
    });
    
    this.topicPool = new DataCollectionPool({
      name: 'TopicMetricsPool', 
      poolSize: this.collectionConfig.topicWorkerPoolSize,
      taskTimeout: 60000
    });
    
    this.consumerPool = new DataCollectionPool({
      name: 'ConsumerMetricsPool',
      poolSize: this.collectionConfig.consumerWorkerPoolSize,
      taskTimeout: 30000
    });
    
    // Load metric definitions
    this.metricDefinitions = getAllMetricDefinitions();
    
    // Collection state
    this.isInitialized = false;
    this.connectionVerified = false;
    this.lastHealthCheck = null;
    
    if (this.debug) {
      logger.debug('EnhancedKafkaCollector initialized with enhanced features');
      logger.debug('Configuration:', this.collectionConfig);
    }
  }

  /**
   * Initialize the collector and verify connections
   */
  async initialize() {
    if (this.isInitialized) {
      return;
    }
    
    logger.info('üöÄ Initializing Enhanced Kafka Collector...');
    
    try {
      // Start worker pools
      this.brokerPool.start();
      this.topicPool.start();
      this.consumerPool.start();
      
      logger.success('‚úÖ Worker pools started');
      
      // Verify Kafka integration
      await this.verifyKafkaConnection();
      
      this.isInitialized = true;
      logger.success('‚úÖ Enhanced Kafka Collector initialized successfully');
      
    } catch (error) {
      logger.error('‚ùå Failed to initialize Enhanced Kafka Collector:', error.message);
      await this.cleanup();
      throw error;
    }
  }

  /**
   * Verify Kafka connection and data availability
   */
  async verifyKafkaConnection() {
    logger.info('üîç Verifying Kafka connection and data availability...');
    
    const startTime = Date.now();
    
    try {
      // Check for recent Kafka data
      const hasData = await this.checkKafkaIntegration();
      
      if (!hasData) {
        throw new Error('No Kafka data found. Ensure nri-kafka is configured and running.');
      }
      
      // Verify we can get broker and topic information
      const clusters = await this.getKafkaClusters();
      
      if (clusters.length === 0) {
        throw new Error('No Kafka clusters found in New Relic data');
      }
      
      logger.success(`‚úÖ Found ${clusters.length} Kafka cluster(s):`);
      clusters.forEach(cluster => {
        logger.info(`   üìä ${cluster.clusterName}: ${cluster.brokerCount} brokers`);
      });
      
      // Test a sample collection
      await this.performConnectionTest();
      
      this.connectionVerified = true;
      this.lastHealthCheck = new Date();
      
      const duration = Date.now() - startTime;
      logger.success(`‚úÖ Kafka connection verified in ${duration}ms`);
      
    } catch (error) {
      logger.error('‚ùå Kafka connection verification failed:', error.message);
      this.connectionVerified = false;
      throw error;
    }
  }

  /**
   * Perform a connection test by collecting sample metrics
   */
  async performConnectionTest() {
    logger.debug('üß™ Performing connection test with sample metric collection...');
    
    try {
      // Test broker metrics collection
      const brokerMetrics = await this.collectKafkaBrokerMetrics('2 minutes ago');
      if (brokerMetrics.length === 0) {
        throw new Error('No broker metrics returned in connection test');
      }
      
      // Test topic metrics collection  
      const topicMetrics = await this.collectKafkaTopicMetrics('2 minutes ago');
      logger.debug(`Connection test: ${brokerMetrics.length} brokers, ${topicMetrics.length} topics`);
      
    } catch (error) {
      logger.error('Connection test failed:', error.message);
      throw new Error(`Connection test failed: ${error.message}`);
    }
  }

  /**
   * Collect comprehensive Kafka metrics using enhanced strategies
   */
  async collectEnhancedKafkaMetrics(since = '5 minutes ago') {
    if (!this.isInitialized) {
      await this.initialize();
    }
    
    logger.info('üìä Collecting enhanced Kafka metrics...');
    const startTime = Date.now();
    
    const results = {
      brokerMetrics: [],
      topicMetrics: [],
      consumerGroupMetrics: [],
      clusterMetrics: [],
      collectionStats: {},
      errors: []
    };
    
    try {
      // Collect different metric types concurrently
      const collectionPromises = [
        this.collectBrokerMetricsEnhanced(since),
        this.collectTopicMetricsEnhanced(since)
      ];
      
      // Add consumer group collection if enabled
      if (this.collectionConfig.enableConsumerLagCollection) {
        collectionPromises.push(this.collectConsumerGroupMetricsEnhanced(since));
      }
      
      // Execute collections concurrently
      const collectionResults = await Promise.allSettled(collectionPromises);
      
      // Process results
      this.processCollectionResults(collectionResults, results);
      
      // Calculate cluster-level aggregations
      if (results.brokerMetrics.length > 0) {
        results.clusterMetrics = await this.calculateClusterMetrics(
          results.brokerMetrics, 
          results.topicMetrics
        );
      }
      
      // Generate collection statistics
      results.collectionStats = this.generateCollectionStats(results, startTime);
      
      // Validate collected metrics
      if (this.collectionConfig.metricValidation) {
        await this.validateCollectedMetrics(results);
      }
      
      logger.success(`‚úÖ Enhanced metric collection completed in ${Date.now() - startTime}ms`);
      this.logCollectionSummary(results);
      
      return results;
      
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error(`‚ùå Enhanced metric collection failed after ${duration}ms: ${error.message}`);
      
      results.errors.push({
        type: 'collection_error',
        error: error.message,
        duration
      });
      
      throw error;
    }
  }

  /**
   * Collect broker metrics with enhanced processing
   */
  async collectBrokerMetricsEnhanced(since) {
    logger.debug('üì° Collecting enhanced broker metrics...');
    
    try {
      // Get base broker data from infrastructure
      const brokerSamples = await this.collectKafkaBrokerMetrics(since);
      
      if (brokerSamples.length === 0) {
        logger.warn('‚ö†Ô∏è No broker samples found');
        return [];
      }
      
      // Extract unique brokers for parallel processing
      const brokers = this.extractUniqueBrokers(brokerSamples);
      
      // Collect detailed metrics for each broker concurrently
      const enhancedBrokerTasks = brokers.map(broker => ({
        taskFn: async () => this.collectDetailedBrokerMetrics(broker, since),
        context: { broker: broker.id, cluster: broker.clusterName }
      }));
      
      const brokerResults = await this.brokerPool.submitBatch(enhancedBrokerTasks);
      
      // Combine base samples with enhanced metrics
      const enhancedMetrics = this.combineBrokerMetrics(brokerSamples, brokerResults);
      
      logger.debug(`‚úÖ Collected enhanced metrics for ${enhancedMetrics.length} brokers`);
      return enhancedMetrics;
      
    } catch (error) {
      logger.error('‚ùå Enhanced broker metrics collection failed:', error.message);
      throw error;
    }
  }

  /**
   * Collect topic metrics with per-broker details
   */
  async collectTopicMetricsEnhanced(since) {
    logger.debug('üìà Collecting enhanced topic metrics...');
    
    try {
      // Get base topic data
      const topicSamples = await this.collectKafkaTopicMetrics(since);
      
      if (topicSamples.length === 0) {
        logger.warn('‚ö†Ô∏è No topic samples found');
        return [];
      }
      
      // If detailed topic metrics are enabled, collect per-broker topic metrics
      if (this.collectionConfig.enableDetailedTopicMetrics) {
        return await this.collectDetailedTopicMetrics(topicSamples, since);
      }
      
      return topicSamples;
      
    } catch (error) {
      logger.error('‚ùå Enhanced topic metrics collection failed:', error.message);
      throw error;
    }
  }

  /**
   * Collect detailed per-broker topic metrics
   */
  async collectDetailedTopicMetrics(topicSamples, since) {
    const topics = this.extractUniqueTopics(topicSamples);
    const brokers = await this.getActiveBrokers(since);
    
    logger.debug(`Collecting detailed metrics for ${topics.length} topics across ${brokers.length} brokers`);
    
    // Create tasks for each topic-broker combination
    const detailedTopicTasks = [];
    
    topics.forEach(topic => {
      brokers.forEach(broker => {
        detailedTopicTasks.push({
          taskFn: async () => this.collectTopicBrokerMetrics(topic, broker, since),
          context: { 
            topic: topic.name, 
            broker: broker.id, 
            cluster: topic.clusterName 
          }
        });
      });
    });
    
    // Collect in batches to avoid overwhelming the system
    const batchSize = 20;
    const enhancedTopicMetrics = [];
    
    for (let i = 0; i < detailedTopicTasks.length; i += batchSize) {
      const batch = detailedTopicTasks.slice(i, i + batchSize);
      const batchResults = await this.topicPool.submitBatch(batch);
      
      const successfulResults = batchResults
        .filter(result => result.status === 'fulfilled')
        .map(result => result.value)
        .filter(Boolean);
        
      enhancedTopicMetrics.push(...successfulResults);
    }
    
    logger.debug(`‚úÖ Collected ${enhancedTopicMetrics.length} detailed topic-broker metrics`);
    return enhancedTopicMetrics;
  }

  /**
   * Collect consumer group metrics with lag calculation  
   */
  async collectConsumerGroupMetricsEnhanced(since) {
    logger.debug('üë• Collecting enhanced consumer group metrics...');
    
    try {
      // This would integrate with consumer offset collector
      // For now, return base consumer metrics if available
      const consumerSamples = await this.collectKafkaConsumerMetrics(since);
      return consumerSamples;
      
    } catch (error) {
      logger.warn('‚ö†Ô∏è Consumer group metrics collection not available:', error.message);
      return [];
    }
  }

  /**
   * Collect consumer metrics (placeholder for integration with consumer offset collector)
   */
  async collectKafkaConsumerMetrics(since) {
    // This will be implemented when we add the consumer offset collector
    // For now, return empty array
    return [];
  }

  /**
   * Collect detailed broker metrics
   */
  async collectDetailedBrokerMetrics(broker, since) {
    const nrql = `
      FROM KafkaBrokerSample 
      SELECT 
        latest(broker.id) as 'broker.id',
        latest(clusterName) as clusterName,
        latest(hostname) as hostname,
        average(broker.bytesInPerSecond) as 'broker.bytesInPerSecond',
        average(broker.bytesOutPerSecond) as 'broker.bytesOutPerSecond',
        average(broker.messagesInPerSecond) as 'broker.messagesInPerSecond',
        average(broker.messagesOutPerSecond) as 'broker.messagesOutPerSecond',
        latest(broker.underReplicatedPartitions) as 'broker.underReplicatedPartitions',
        latest(broker.offlinePartitionsCount) as 'broker.offlinePartitionsCount',
        average(broker.requestHandlerAvgIdlePercent) as 'broker.requestHandlerIdlePercent',
        average(net.networkProcessorAvgIdlePercent) as 'broker.networkProcessorIdlePercent',
        average(net.requestsPerSecond) as 'broker.requestsPerSecond',
        average(broker.IOWaitPercent) as 'broker.IOWaitPercent',
        average(disk.usedPercent) as 'disk.usedPercent'
      WHERE broker.id = ${broker.id} AND clusterName = '${broker.clusterName}'
      SINCE ${since}
    `;

    try {
      const query = `
        {
          actor {
            account(id: ${this.accountId}) {
              nrql(query: "${nrql.replace(/\n/g, ' ').replace(/"/g, '\\"')}") {
                results
              }
            }
          }
        }
      `;

      const response = await this.nerdGraphQuery(query);
      const results = response.data?.actor?.account?.nrql?.results || [];
      
      return results.map(result => ({
        eventType: 'KafkaBrokerSample',
        ...result,
        _enhanced: true
      }));
      
    } catch (error) {
      logger.warn(`Failed to collect detailed metrics for broker ${broker.id}:`, error.message);
      return null;
    }
  }

  /**
   * Collect topic metrics for specific topic-broker combination
   */
  async collectTopicBrokerMetrics(topic, broker, since) {
    // This would collect per-topic per-broker metrics
    // For now, return the topic metrics we already have
    return {
      eventType: 'KafkaTopicSample',
      'topic.name': topic.name,
      clusterName: topic.clusterName,
      'broker.id': broker.id,
      _enhanced: true,
      _collection: 'topic-broker-level'
    };
  }

  /**
   * Calculate cluster-level metrics from broker and topic data
   */
  async calculateClusterMetrics(brokerMetrics, topicMetrics) {
    const clusterGroups = {};
    
    // Group by cluster
    brokerMetrics.forEach(metric => {
      const cluster = metric.clusterName || 'default';
      if (!clusterGroups[cluster]) {
        clusterGroups[cluster] = { brokers: [], topics: [] };
      }
      clusterGroups[cluster].brokers.push(metric);
    });
    
    topicMetrics.forEach(metric => {
      const cluster = metric.clusterName || 'default';
      if (!clusterGroups[cluster]) {
        clusterGroups[cluster] = { brokers: [], topics: [] };
      }
      clusterGroups[cluster].topics.push(metric);
    });
    
    // Calculate aggregated metrics for each cluster
    const clusterMetrics = [];
    
    Object.entries(clusterGroups).forEach(([clusterName, data]) => {
      const aggregated = this.aggregateClusterMetrics(clusterName, data.brokers, data.topics);
      clusterMetrics.push(aggregated);
    });
    
    return clusterMetrics;
  }

  /**
   * Aggregate broker and topic metrics into cluster metrics
   */
  aggregateClusterMetrics(clusterName, brokers, topics) {
    const totals = {
      messagesIn: 0,
      messagesOut: 0,
      bytesIn: 0,
      bytesOut: 0,
      underReplicatedPartitions: 0,
      offlinePartitions: 0
    };
    
    brokers.forEach(broker => {
      totals.messagesIn += broker['broker.messagesInPerSecond'] || 0;
      totals.messagesOut += broker['broker.messagesOutPerSecond'] || 0;
      totals.bytesIn += broker['broker.bytesInPerSecond'] || 0;
      totals.bytesOut += broker['broker.bytesOutPerSecond'] || 0;
      totals.underReplicatedPartitions += broker['broker.underReplicatedPartitions'] || 0;
      totals.offlinePartitions += broker['broker.offlinePartitionsCount'] || 0;
    });
    
    return {
      eventType: 'KafkaClusterSample',
      clusterName: clusterName,
      'cluster.brokerCount': brokers.length,
      'cluster.topicCount': topics.length,
      'cluster.messagesInPerSecond': totals.messagesIn,
      'cluster.messagesOutPerSecond': totals.messagesOut,
      'cluster.bytesInPerSecond': totals.bytesIn,
      'cluster.bytesOutPerSecond': totals.bytesOut,
      'cluster.underReplicatedPartitions': totals.underReplicatedPartitions,
      'cluster.offlinePartitions': totals.offlinePartitions,
      'cluster.health.score': this.calculateClusterHealthScore(totals, brokers.length),
      '_enhanced': true,
      '_aggregated': true
    };
  }

  /**
   * Calculate cluster health score
   */
  calculateClusterHealthScore(totals, brokerCount) {
    let score = 100;
    
    // Deduct for offline partitions (critical)
    if (totals.offlinePartitions > 0) {
      score -= Math.min(50, totals.offlinePartitions * 10);
    }
    
    // Deduct for under-replicated partitions
    if (totals.underReplicatedPartitions > 0) {
      score -= Math.min(30, totals.underReplicatedPartitions * 2);
    }
    
    // Deduct for low throughput (might indicate issues)
    if (totals.messagesIn < 10 && brokerCount > 0) {
      score -= 10;
    }
    
    return Math.max(0, score);
  }

  /**
   * Extract unique brokers from samples
   */
  extractUniqueBrokers(samples) {
    const brokerMap = new Map();
    
    samples.forEach(sample => {
      const key = `${sample['broker.id']}-${sample.clusterName}`;
      if (!brokerMap.has(key)) {
        brokerMap.set(key, {
          id: sample['broker.id'],
          clusterName: sample.clusterName,
          hostname: sample.hostname
        });
      }
    });
    
    return Array.from(brokerMap.values());
  }

  /**
   * Extract unique topics from samples
   */
  extractUniqueTopics(samples) {
    const topicMap = new Map();
    
    samples.forEach(sample => {
      const key = `${sample['topic.name']}-${sample.clusterName}`;
      if (!topicMap.has(key)) {
        topicMap.set(key, {
          name: sample['topic.name'],
          clusterName: sample.clusterName
        });
      }
    });
    
    return Array.from(topicMap.values());
  }

  /**
   * Get active brokers for detailed collection
   */
  async getActiveBrokers(since) {
    const brokerMetrics = await this.collectKafkaBrokerMetrics(since);
    return this.extractUniqueBrokers(brokerMetrics);
  }

  /**
   * Combine base broker samples with enhanced metrics
   */
  combineBrokerMetrics(baseSamples, enhancedResults) {
    const enhanced = enhancedResults
      .filter(result => result.status === 'fulfilled')
      .map(result => result.value)
      .filter(Boolean)
      .flat();
    
    // If we have enhanced metrics, prefer those; otherwise use base samples
    return enhanced.length > 0 ? enhanced : baseSamples;
  }

  /**
   * Process collection results from Promise.allSettled
   */
  processCollectionResults(collectionResults, results) {
    collectionResults.forEach((result, index) => {
      if (result.status === 'fulfilled') {
        const data = result.value;
        switch (index) {
          case 0: // Broker metrics
            results.brokerMetrics = data;
            break;
          case 1: // Topic metrics  
            results.topicMetrics = data;
            break;
          case 2: // Consumer group metrics
            results.consumerGroupMetrics = data;
            break;
        }
      } else {
        results.errors.push({
          type: `collection_${index}`,
          error: result.reason.message
        });
      }
    });
  }

  /**
   * Generate collection statistics
   */
  generateCollectionStats(results, startTime) {
    const duration = Date.now() - startTime;
    
    return {
      totalDuration: duration,
      brokerCount: results.brokerMetrics.length,
      topicCount: results.topicMetrics.length,
      consumerGroupCount: results.consumerGroupMetrics.length,
      clusterCount: results.clusterMetrics.length,
      errorCount: results.errors.length,
      workerPoolStats: {
        broker: this.brokerPool.getStatus(),
        topic: this.topicPool.getStatus(),
        consumer: this.consumerPool.getStatus()
      }
    };
  }

  /**
   * Validate collected metrics against definitions
   */
  async validateCollectedMetrics(results) {
    if (!this.collectionConfig.metricValidation) {
      return;
    }
    
    const validationErrors = [];
    
    // Validate broker metrics
    results.brokerMetrics.forEach((metric, index) => {
      const errors = this.validateBrokerMetric(metric);
      if (errors.length > 0) {
        validationErrors.push({ type: 'broker', index, errors });
      }
    });
    
    // Validate topic metrics
    results.topicMetrics.forEach((metric, index) => {
      const errors = this.validateTopicMetric(metric);
      if (errors.length > 0) {
        validationErrors.push({ type: 'topic', index, errors });
      }
    });
    
    if (validationErrors.length > 0 && this.debug) {
      logger.warn(`‚ö†Ô∏è ${validationErrors.length} metric validation warnings`);
      validationErrors.slice(0, 5).forEach(error => {
        logger.debug(`Validation: ${error.type}[${error.index}]: ${error.errors.join(', ')}`);
      });
    }
  }

  /**
   * Validate individual broker metric
   */
  validateBrokerMetric(metric) {
    const errors = [];
    const brokerMetrics = this.metricDefinitions.broker;
    
    Object.entries(brokerMetrics).forEach(([metricName, definition]) => {
      const value = metric[metricName];
      if (value !== undefined) {
        const validation = definition.validate(value);
        if (!validation.valid) {
          errors.push(`${metricName}: ${validation.reason}`);
        }
      }
    });
    
    return errors;
  }

  /**
   * Validate individual topic metric
   */
  validateTopicMetric(metric) {
    const errors = [];
    const topicMetrics = this.metricDefinitions.topic;
    
    Object.entries(topicMetrics).forEach(([metricName, definition]) => {
      const value = metric[metricName];
      if (value !== undefined) {
        const validation = definition.validate(value);
        if (!validation.valid) {
          errors.push(`${metricName}: ${validation.reason}`);
        }
      }
    });
    
    return errors;
  }

  /**
   * Log collection summary
   */
  logCollectionSummary(results) {
    const stats = results.collectionStats;
    
    logger.info('üìä Collection Summary:');
    logger.info(`   ‚è±Ô∏è  Duration: ${stats.totalDuration}ms`);
    logger.info(`   üñ•Ô∏è  Brokers: ${stats.brokerCount}`);
    logger.info(`   üìà Topics: ${stats.topicCount}`);
    logger.info(`   üë• Consumer Groups: ${stats.consumerGroupCount}`);
    logger.info(`   üåê Clusters: ${stats.clusterCount}`);
    
    if (stats.errorCount > 0) {
      logger.warn(`   ‚ùå Errors: ${stats.errorCount}`);
    }
    
    // Log worker pool performance
    Object.entries(stats.workerPoolStats).forEach(([poolName, poolStats]) => {
      if (poolStats.metrics.tasksProcessed > 0) {
        logger.debug(`   üîß ${poolName} pool: ${poolStats.metrics.tasksProcessed} tasks, avg ${poolStats.metrics.avgProcessingTime.toFixed(0)}ms`);
      }
    });
  }

  /**
   * Get comprehensive health status
   */
  async getHealthStatus() {
    const status = {
      initialized: this.isInitialized,
      connectionVerified: this.connectionVerified,
      lastHealthCheck: this.lastHealthCheck,
      workerPools: {
        broker: this.brokerPool.getStatus(),
        topic: this.topicPool.getStatus(),
        consumer: this.consumerPool.getStatus()
      }
    };
    
    // Check if health check is stale
    if (this.lastHealthCheck) {
      const timeSinceCheck = Date.now() - this.lastHealthCheck.getTime();
      status.healthCheckStale = timeSinceCheck > 300000; // 5 minutes
    }
    
    return status;
  }

  /**
   * Cleanup resources
   */
  async cleanup() {
    logger.info('üßπ Cleaning up Enhanced Kafka Collector...');
    
    try {
      await Promise.all([
        this.brokerPool.stop(),
        this.topicPool.stop(), 
        this.consumerPool.stop()
      ]);
      
      this.isInitialized = false;
      this.connectionVerified = false;
      
      logger.success('‚úÖ Enhanced Kafka Collector cleanup completed');
      
    } catch (error) {
      logger.error('‚ùå Cleanup failed:', error.message);
    }
  }
}

module.exports = EnhancedKafkaCollector;